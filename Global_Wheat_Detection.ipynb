{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XefFoAGJ_w6f"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageEnhance\n",
    "import albumentations as albu\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82WqtVqP_JLm"
   },
   "source": [
    "Images to Download from : https://drive.google.com/drive/folders/1nuiuUfCOp2X_FTr28-63sOOx4q_iF0EX?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-OuVETE_hoa"
   },
   "source": [
    "**Problem Statement**\n",
    "\n",
    "\n",
    "Open up your pantry and you’re likely to find several wheat products. Indeed, your morning toast or cereal may rely upon this common grain. Its popularity as a food and crop makes wheat widely studied. To get large and accurate data about wheat fields worldwide, plant scientists use image detection of \"wheat heads\"—spikes atop the plant containing grain. These images are used to estimate the density and size of wheat heads in different varieties. Farmers can use the data to assess health and maturity when making management decisions in their fields.\n",
    "\n",
    "However, accurate wheat head detection in outdoor field images can be visually challenging. There is often overlap of dense wheat plants, and the wind can blur the photographs. Both make it difficult to identify single heads. Additionally, appearances vary due to maturity, color, genotype, and head orientation. Finally, because wheat is grown worldwide, different varieties, planting densities, patterns, and field conditions must be considered. Models developed for wheat phenotyping need to generalize between different growing environments. Current detection methods involve one- and two-stage detectors (Yolo-V3 and Faster-RCNN), but even when trained with a large dataset, a bias to the training region remains.\n",
    "\n",
    "The Global Wheat Head Dataset is led by nine research institutes from seven countries: the University of Tokyo, Institut national de recherche pour l’agriculture, l’alimentation et l’environnement, Arvalis, ETHZ, University of Saskatchewan, University of Queensland, Nanjing Agricultural University, and Rothamsted Research. These institutions are joined by many in their pursuit of accurate wheat head detection, including the Global Institute for Food Security, DigitAg, Kubota, and Hiphen.\n",
    "\n",
    "In this competition, you’ll detect wheat heads from outdoor images of wheat plants, including wheat datasets from around the globe. Using worldwide data, you will focus on a generalized solution to estimate the number and size of wheat heads. To better gauge the performance for unseen genotypes, environments, and observational conditions, the training dataset covers multiple regions. You will use more than 3,000 images from Europe (France, UK, Switzerland) and North America (Canada). The test data includes about 1,000 images from Australia, Japan, and China.\n",
    "\n",
    "Wheat is a staple across the globe, which is why this competition must account for different growing conditions. Models developed for wheat phenotyping need to be able to generalize between environments. If successful, researchers can accurately estimate the density and size of wheat heads in different varieties. With improved detection farmers can better assess their crops, ultimately bringing cereal, toast, and other favorite dishes to your table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "id": "po0HV0qV_7rc",
    "outputId": "f2b4ac13-09b5-4166-bc15-2ccc0af25562"
   },
   "outputs": [],
   "source": [
    "labels = pd.read_csv('/content/drive/My Drive/Global Wheat Detection/Global Wheat Detection/train.csv/train.csv')\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unE3-Qu7_pC0"
   },
   "source": [
    "In the raw data each bounding box is in an array of length 4 but formatted as a string. The below block of code groups the bounding boxes by image id and places the bounding boxes as numpy arrays next to each image id. The image id can then be used to quickly retrieve all of the bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jFlyqAjJAGZ3"
   },
   "outputs": [],
   "source": [
    "def group_boxes(group):\n",
    "    boundaries = group['bbox'].str.split(',', expand=True)\n",
    "    boundaries[0] = boundaries[0].str.slice(start=1)\n",
    "    boundaries[3] = boundaries[3].str.slice(stop=-1)\n",
    "    \n",
    "    return boundaries.values.astype(float)\n",
    "\n",
    "labels = labels.groupby('image_id').apply(group_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "id": "6Fa76Vj_4R7_",
    "outputId": "5587c318-81e8-4c5a-b3a3-cbb553d4d317"
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8LPmMQVAm_1"
   },
   "source": [
    "Here's a sample of five bounding boxes for one of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "dcAUZP5JAn3m",
    "outputId": "b678b47e-f408-4ae4-a4cf-7666bf761a0a"
   },
   "outputs": [],
   "source": [
    "labels['00333207f'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efIf-PLu_pC9"
   },
   "source": [
    "With the labels extracted from the data, now we need the images loaded as numpy arrays. At this point it is worth splitting the data into a training and validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVk0ZL4KAr42"
   },
   "outputs": [],
   "source": [
    "train_image_ids = np.unique(labels.index.values)[0:3363]\n",
    "val_image_ids = np.unique(labels.index.values)[3363:3373]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxZ9rXTVBZJg"
   },
   "source": [
    "With the ids split it's now time to load the images. To keep the training of the model relatively fast I will resize each image from (1024,1024) to (256,256). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8AVVsXIUBQrl"
   },
   "outputs": [],
   "source": [
    "def load_image(path,image_id):\n",
    "    image = Image.open(path + image_id + \".jpg\")\n",
    "    image = image.resize((256, 256))\n",
    "    \n",
    "    return np.asarray(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_k4l7mQBnyC"
   },
   "source": [
    "Define path variable \n",
    "\n",
    "```\n",
    "\n",
    "path = '/content/drive/My Drive/Global Wheat Detection/Global Wheat Detection/train/'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LdlOMfNOByoh"
   },
   "outputs": [],
   "source": [
    "path ='/content/drive/My Drive/Global Wheat Detection/Global Wheat Detection/train/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "f94a386a48534acdbd2554ae285ca69c",
      "f84211a5e13d4c1a8bebc29a6ff76424",
      "1b87383d833949f39a7e0baa23092b83",
      "958d613a534645a7ae555407c273691a",
      "4deacb4d99f84cfd8af95d5a487b7cca",
      "ed3c2c99d4c14a8fb426635085e59dee",
      "0917a3922b9b40f0929c24c94ff1d18c",
      "daa1a252e34f4eec9dd733d85bbf6954"
     ]
    },
    "id": "2lDLcFj_BkcJ",
    "outputId": "1618bd51-29fb-4ac1-acad-1869f46ed061"
   },
   "outputs": [],
   "source": [
    "train_pixels = {}\n",
    "train_labels = {}\n",
    "\n",
    "for image_id in tqdm(train_image_ids):\n",
    "    train_pixels[image_id] = load_image(path,image_id)\n",
    "    train_labels[image_id] = labels[image_id].copy() / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "98764f9cf50545f38d2d601c567fbb72",
      "f37cd14598744b1d9c590400ae8054c6",
      "9f39bcc48581431aafd02bb8713cbe0a",
      "dfda22bf83524d0ca26f0f3b6d89e051",
      "46c79133c0f042c1a9d5b811374bc047",
      "ed622829fe5944ddb56672caaf4d7c73",
      "9f0c9acd75714703a2421a8bb03de809",
      "0cfab5ab0637424c9ac7f8c0fef5e4cc"
     ]
    },
    "id": "KlyLCPdzAdPB",
    "outputId": "60087e3c-3ce3-4c77-fdc1-4122b8582b2d"
   },
   "outputs": [],
   "source": [
    "val_pixels = {}\n",
    "val_labels = {}\n",
    "\n",
    "for image_id in tqdm(val_image_ids):\n",
    "    val_pixels[image_id] = load_image(path,image_id)    \n",
    "    val_labels[image_id] = labels[image_id].copy() / 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-F-1ayB_JMK-"
   },
   "source": [
    "## Visualise images\n",
    "\n",
    "Before going on it is worth having a look at some of the images and bounding boxes in the dataset. For that a few helper functions will be required. The below functions take an image id and the corresponding bounding boxes and return the bounding boxes drawn onto the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XWTGEsu1I1ZP"
   },
   "outputs": [],
   "source": [
    "Path ='/content/drive/My Drive/Global Wheat Detection/Global Wheat Detection/'\n",
    "\n",
    "def draw_bboxes(image_id, bboxes, source='train'):  \n",
    "    image = Image.open(Path + source +'/' + image_id + \".jpg\")\n",
    "    image = image.resize((256,256))\n",
    "    \n",
    "    draw = ImageDraw.Draw(image)\n",
    "            \n",
    "    for bbox in bboxes:\n",
    "        draw_bbox(draw, bbox)\n",
    "    \n",
    "    return np.asarray(image)\n",
    "\n",
    "\n",
    "def draw_bbox(draw, bbox):\n",
    "    x, y, width, height = bbox\n",
    "    draw.rectangle([x, y, x + width, y + height], width=2, outline='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nXTnsMhANMDi"
   },
   "outputs": [],
   "source": [
    "def show_images(image_ids, bboxes, source='train'):\n",
    "    pixels = []\n",
    "    \n",
    "    for image_id in image_ids:\n",
    "        pixels.append(\n",
    "            draw_bboxes(image_id, bboxes[image_id], source)\n",
    "        )\n",
    "    \n",
    "    num_of_images = len(image_ids)\n",
    "    fig, axes = plt.subplots(\n",
    "        1, \n",
    "        num_of_images, \n",
    "        figsize=(5 * num_of_images, 5 * num_of_images)\n",
    "    )\n",
    "    \n",
    "    for i, image_pixels in enumerate(pixels):\n",
    "        axes[i].imshow(image_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "id": "XCNIR9sJP-XJ",
    "outputId": "25385897-53ca-4a9b-aa4f-b29f41ad279b"
   },
   "outputs": [],
   "source": [
    "show_images(train_image_ids[0:3], train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQD-n7vb_pDY"
   },
   "source": [
    "## Clean bounding boxes\n",
    "\n",
    "There are a small number of bounding boxes in this dataset that do not bound a head of wheat. While the number is small enough that the model can still learn how to detect the heads of wheat they still cause a little bit of inaccuracy. Below I'll search for tiny bounding boxes that cannot possibly fit a head of wheat inside them and huge bounding boxes that miss the head of wheat they are aimed at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "-9HkaemMQtjt",
    "outputId": "3fb27583-85df-40f9-f005-7c3112558ad1"
   },
   "outputs": [],
   "source": [
    "tiny_bboxes = []\n",
    "\n",
    "for i, image_id in enumerate(train_image_ids):\n",
    "    for label in train_labels[image_id]:\n",
    "        if label[2] * label[3] <= 10 and label[2] * label[3] != 0:\n",
    "            tiny_bboxes.append(i)\n",
    "\n",
    "            \n",
    "print(str(len(tiny_bboxes)) + ' tiny bounding boxes found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "rN994HKgRnsY",
    "outputId": "7e833d68-b038-413a-8753-327188a7ccc2"
   },
   "outputs": [],
   "source": [
    "huge_bboxes = []\n",
    "\n",
    "for i, image_id in enumerate(train_image_ids):\n",
    "    for label in train_labels[image_id]:\n",
    "        if label[2] * label[3] > 8000:\n",
    "            huge_bboxes.append(i)\n",
    "\n",
    "            \n",
    "print(str(len(huge_bboxes)) + ' huge bounding boxes found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCLvaemw_pDe"
   },
   "source": [
    "The tiny bounding boxes are actually too small to show when visualised on an image. However we can take a peak at one of the huge bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "shGj6r78Rt3h",
    "outputId": "22cfd16e-7908-46fb-ecc3-8f112e434d3c"
   },
   "outputs": [],
   "source": [
    "show_images(train_image_ids[562:564], train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CdX4no4_pDi"
   },
   "source": [
    "I did some more manual inspection of the bad labels picked out of this code that I have not included in this notebook. I found that some huge bounding boxes were actually okay as they bound a very zoomed in image. To this end I have listed a few to be kept (1079, 1371, 2020). Otherwise the below code throws out any bounding boxes whose area is larger than 8000 or smaller than 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4DgnTQSR47h"
   },
   "outputs": [],
   "source": [
    "def clean_labels(train_image_ids, train_labels):\n",
    "    good_labels = {}\n",
    "    \n",
    "    for i, image_id in enumerate(train_image_ids):\n",
    "        good_labels[image_id] = []\n",
    "        \n",
    "        for j, label in enumerate(train_labels[image_id]):\n",
    "\n",
    "            # remove huge bbox\n",
    "            if label[2] * label[3] > 8000 and i not in [1079, 1371, 2020]:\n",
    "                continue\n",
    "\n",
    "            # remove tiny bbox\n",
    "            elif label[2] < 5 or label[3] < 5:\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                good_labels[image_id].append(\n",
    "                    train_labels[image_id][j]\n",
    "                )\n",
    "                \n",
    "    return good_labels\n",
    "\n",
    "train_labels = clean_labels(train_image_ids, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBXJMDvy_pDm"
   },
   "source": [
    "## Data pipeline\n",
    "\n",
    "Usually I would use Tensorflows data api or keras data generators to build a pipeline to get data into the model. However the pre-processing that needs to be done for this model is not trivial and it turned out to be easier to create a custom data generator. This takes the form of a class that is passed to keras' fit generator function. It contains the following functionality:\n",
    "\n",
    "- define the size of the dataset. Keras needs this to work out how long an epoch is.\n",
    "- shuffle the dataset.\n",
    "- get an image and augment it to add variety to the dataset. This includes amending bounding boxes when a head of wheat has changed in the image.\n",
    "- reshape the bounding boxes to a label grid.\n",
    "\n",
    "I'll start by initialising the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "5kFZteEQUKbB",
    "outputId": "e9bc6f41-97a6-4749-c18b-38406c39920a"
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "for j in range(1,16):\n",
    "  i+=16\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZG4mJqRUN16"
   },
   "outputs": [],
   "source": [
    "image_grid = np.zeros((16, 16, 4))\n",
    "cell = [0, 0, 256 / 16, 256 / 16]\n",
    "for i in range(0, 16):\n",
    "  for j in range(0, 16):\n",
    "    image_grid[i,j] = cell\n",
    "    cell[0] = cell[0] + cell[2]\n",
    "  cell[0] = 0\n",
    "  cell[1] = cell[1] + cell[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 887
    },
    "id": "LPAC3S4fWkPk",
    "outputId": "5d304bf1-147b-4fed-fc2a-4c1daa820170"
   },
   "outputs": [],
   "source": [
    "image_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oHvlJM69SZxl"
   },
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, image_ids, image_pixels, labels=None, batch_size=1, shuffle=False, augment=False):\n",
    "        self.image_ids = image_ids\n",
    "        self.image_pixels = image_pixels\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.augment = augment\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        self.image_grid = self.form_image_grid()\n",
    "        \n",
    "        \n",
    "    def form_image_grid(self):    \n",
    "        image_grid = np.zeros((16, 16, 4))\n",
    "\n",
    "        # x, y, width, height\n",
    "        cell = [0, 0, 256 / 16, 256 / 16] \n",
    "\n",
    "        for i in range(0, 16):\n",
    "            for j in range(0, 16):\n",
    "                image_grid[i,j] = cell\n",
    "\n",
    "                cell[0] = cell[0] + cell[2]\n",
    "\n",
    "            cell[0] = 0\n",
    "            cell[1] = cell[1] + cell[3]\n",
    "\n",
    "        return image_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRMVJnKC_pDs"
   },
   "source": [
    "Next I will add some methods to the class that keras needs to operate the data generation. length is used to determine how many images there are in the dataset. on_epoch_end is called at the end of each epoch (as well once before training starts) to get the index of all images in the dataset. It is also has the opportunity to shuffle the dataset per epoch if the generator was configured to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0yFTTqR_pDs"
   },
   "outputs": [],
   "source": [
    "def __len__(self):\n",
    "    return int(np.floor(len(self.image_ids) / self.batch_size))\n",
    "\n",
    "\n",
    "def on_epoch_end(self):\n",
    "    self.indexes = np.arange(len(self.image_ids))\n",
    "\n",
    "    if self.shuffle == True:\n",
    "        np.random.shuffle(self.indexes)\n",
    "\n",
    "\n",
    "DataGenerator.__len__ = __len__\n",
    "DataGenerator.on_epoch_end = on_epoch_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46H_Aelz_pDu"
   },
   "source": [
    "Regarding the augmentations a number of transformations will be applied to each training image before they are fed into the model. This helps to add some diversity to a small dataset effectively growing it to a much larger one:\n",
    "\n",
    "- **random sized crop:** The model needs to be able to detect a wheat head regardless of how close or far away the head is to the camera. To produce more zoom levels in the dataset the crop method will take a portion of the image and zoom in to create a new image with larger wheat heads.\n",
    "- **flip amd rotate**: The wheat heads can point in any direction. To create more examples of wheat heads pointing in different directions the image will randomly be flipped both horizontally and vertically or rotated.\n",
    "- **hue saturation and brightness:** these are various methods that will alter the lighting of the image which will help to create different lighting scenarios. This helps as the test pictures are from various countries each with their own lighting levels.\n",
    "- **noise:** Some wheat heads aren't quite in focus. Adding some noise to the images helps to catch these wheat heads while also forcing the model to learn more abstract wheat head shapes. This helps a lot with over-fitting.\n",
    "- **cutout**: randomly remove small squares of pixels in the image. This prevents the model simply memorizing certain wheat heads and instead forces it to learn the patterns that represent a wheat head.\n",
    "- **clahe:** this is a must have. In many images the wheat heads are a similar colour to the grass in the background making it tricky for the model to differentiate between them. CLAHE helps to exemplify the colour difference between the two.\n",
    "- **grey scale:** I found that there were a few images with a yellow/gold tint. My model was learning to detect wheat heads without a tint (as most images do not contain a tint) and was really struggling to detect anything on the yellow images. By converting all images to grey scale the model is forced to ignore these tints making it much more effective at identifying wheat heads regardless of tint.\n",
    "\n",
    "I also greyscale and apply CLAHE to each validation image as the model has learnt on grey images where the wheat heads are given a lighter shade of grey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95fZEGl__pDv"
   },
   "outputs": [],
   "source": [
    "DataGenerator.train_augmentations = albu.Compose([\n",
    "        albu.RandomSizedCrop(\n",
    "            min_max_height=(200, 200), \n",
    "            height=256, \n",
    "            width=256, \n",
    "            p=0.8\n",
    "        ),\n",
    "        albu.OneOf([\n",
    "            albu.Flip(),\n",
    "            albu.RandomRotate90(),\n",
    "        ], p=1),\n",
    "        albu.OneOf([\n",
    "            albu.HueSaturationValue(),\n",
    "            albu.RandomBrightnessContrast()\n",
    "        ], p=1),\n",
    "        albu.OneOf([\n",
    "            albu.GaussNoise()\n",
    "        ], p=0.5),\n",
    "        albu.Cutout(\n",
    "            num_holes=8, \n",
    "            max_h_size=16, \n",
    "            max_w_size=16, \n",
    "             \n",
    "            p=0.5\n",
    "        ),\n",
    "        albu.CLAHE(p=1),\n",
    "        albu.ToGray(p=1),\n",
    "    ], \n",
    "    bbox_params={'format': 'coco', 'label_fields': ['labels']})\n",
    "\n",
    "DataGenerator.val_augmentations = albu.Compose([\n",
    "    albu.CLAHE(p=1),\n",
    "    albu.ToGray(p=1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_OBl7Y7_pDx"
   },
   "source": [
    "The next finctions load an image and the corresponding bounding boxes depending on randomly picked image ids. As well as loading the images the above augmentations are added to an image as it is loaded. As the albumentaitons library was used the apply these augmentations I get the bounding boxes re-sized for free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CijW2a7K_pDx"
   },
   "outputs": [],
   "source": [
    "def __getitem__(self, index):\n",
    "    indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "    batch_ids = [self.image_ids[i] for i in indexes]\n",
    "\n",
    "    X, y = self.__data_generation(batch_ids)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def __data_generation(self, batch_ids):\n",
    "    X, y = [], []\n",
    "\n",
    "    # Generate data\n",
    "    for i, image_id in enumerate(batch_ids):\n",
    "        pixels = self.image_pixels[image_id]\n",
    "        bboxes = self.labels[image_id]\n",
    "\n",
    "        if self.augment:     \n",
    "            pixels, bboxes = self.augment_image(pixels, bboxes)\n",
    "        else:\n",
    "            pixels = self.contrast_image(pixels)\n",
    "            bboxes = self.form_label_grid(bboxes)\n",
    "\n",
    "        X.append(pixels)\n",
    "        y.append(bboxes)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def augment_image(self, pixels, bboxes):\n",
    "    bbox_labels = np.ones(len(bboxes))\n",
    "\n",
    "    aug_result = self.train_augmentations(image=pixels, bboxes=bboxes, labels=bbox_labels)\n",
    "\n",
    "    bboxes = self.form_label_grid(aug_result['bboxes'])\n",
    "\n",
    "    return np.array(aug_result['image']) / 255, bboxes\n",
    "\n",
    "\n",
    "def contrast_image(self, pixels):        \n",
    "    aug_result = self.val_augmentations(image=pixels)\n",
    "    return np.array(aug_result['image']) / 255\n",
    "\n",
    "\n",
    "DataGenerator.__getitem__ = __getitem__\n",
    "DataGenerator.__data_generation = __data_generation\n",
    "DataGenerator.augment_image = augment_image\n",
    "DataGenerator.contrast_image = contrast_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRRb6Y-L_pD0"
   },
   "source": [
    "The final part of the data generator class re-shapes the bounding box labels. It's worth mentioning here that there are a number of ways to represent a bounding box with four numbers. Some common ways are coco (the shape the boxes are in the raw data), voc-pascal and yolo.\n",
    "\n",
    "\n",
    "\n",
    "I'll be using the yolo shape for this model. In addition to the above shape, yolo detects objects by placing a grid over the image and asking if an object (such as a wheat head) is present in any of the cells of the grid. I've decided to use a 16x16 grid for this challenge which I'll refer to as a label grid. The bounding boxes are reshaped to be offset within the relevant cells of the image. Then all four variables (x, y, width and height) are scaled down to a 0-1 scale using the width and height of the image.\n",
    "\n",
    "\n",
    "It's x and y are offset to the cell while width and height remain the same*\n",
    "\n",
    "Any cells in the grid that have no objects within them contain a bounding box with the dimensions [0, 0, 0, 0].\n",
    "\n",
    "Each bounding box gets a confidence score where a value of 1 tells us that an object (wheat head) is present in the cell and a value of 0 tells us that no object is present. So a cell with an object present could contain a value like this: [1, 0.5, 0.5, 0.2, 0.2] telling us that there is an object present (due to the confidence score of 1, the centre of the bounding box is exactly in the middle of the cell and the box is 20% of the images total width and height.\n",
    "\n",
    "As a cell could contain two overlapping heads of wheat I have configured the grid to contain up to two bounding boxes. These are known as anchor boxes.\n",
    "\n",
    "The below code takes the list of bounding boxes for an image and puts them into the yolo label grid shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "diSUsENCU7ke"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3sbvmBb_pD0"
   },
   "outputs": [],
   "source": [
    "def form_label_grid(self, bboxes):\n",
    "    label_grid = np.zeros((16, 16, 10))\n",
    "\n",
    "    for i in range(0, 16):\n",
    "        for j in range(0, 16):\n",
    "            cell = self.image_grid[i,j]\n",
    "            label_grid[i,j] = self.rect_intersect(cell, bboxes)\n",
    "\n",
    "    return label_grid\n",
    "\n",
    "\n",
    "def rect_intersect(self, cell, bboxes): \n",
    "    cell_x, cell_y, cell_width, cell_height = cell\n",
    "    cell_x_max = cell_x + cell_width \n",
    "    cell_y_max = cell_y + cell_height\n",
    "    \n",
    "    anchor_one = np.array([0, 0, 0, 0, 0])\n",
    "    anchor_two = np.array([0, 0, 0, 0, 0])\n",
    "\n",
    "    # check all boxes\n",
    "    for bbox in bboxes:\n",
    "        box_x, box_y, box_width, box_height = bbox\n",
    "        box_x_centre = box_x + (box_width / 2)\n",
    "        box_y_centre = box_y + (box_height / 2)\n",
    "\n",
    "        if(box_x_centre >= cell_x and box_x_centre < cell_x_max and box_y_centre >= cell_y and box_y_centre < cell_y_max):\n",
    "            \n",
    "            if anchor_one[0] == 0:\n",
    "                anchor_one = self.yolo_shape(\n",
    "                    [box_x, box_y, box_width, box_height], \n",
    "                    [cell_x, cell_y, cell_width, cell_height]\n",
    "                )\n",
    "            \n",
    "            if anchor_one[0] != 0 and anchor_two[0] == 0:\n",
    "                anchor_two = self.yolo_shape(\n",
    "                    [box_x, box_y, box_width, box_height], \n",
    "                    [cell_x, cell_y, cell_width, cell_height]\n",
    "                )\n",
    "                \n",
    "            if anchor_one[0] != 0 and anchor_two[0] != 0:\n",
    "                break\n",
    "\n",
    "    return np.concatenate((anchor_one, anchor_two), axis=None)\n",
    "\n",
    "\n",
    "def yolo_shape(self, box, cell):\n",
    "    box_x, box_y, box_width, box_height = box\n",
    "    cell_x, cell_y, cell_width, cell_height = cell\n",
    "\n",
    "    # top left x,y to centre x,y\n",
    "    box_x = box_x + (box_width / 2)\n",
    "    box_y = box_y + (box_height / 2)\n",
    "\n",
    "    # offset bbox x,y to cell x,y\n",
    "    box_x = (box_x - cell_x) / cell_width\n",
    "    box_y = (box_y - cell_y) / cell_height\n",
    "\n",
    "    # bbox width,height relative to cell width,height\n",
    "    box_width = box_width / 256\n",
    "    box_height = box_height / 256\n",
    "\n",
    "    return [1, box_x, box_y, box_width, box_height]\n",
    "\n",
    "\n",
    "DataGenerator.form_label_grid = form_label_grid\n",
    "DataGenerator.rect_intersect = rect_intersect\n",
    "DataGenerator.yolo_shape = yolo_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hs2EAiyzTQdV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D8W4cKBf_pD3"
   },
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(\n",
    "    train_image_ids,\n",
    "    train_pixels,\n",
    "    train_labels, \n",
    "    batch_size=8, \n",
    "    shuffle=True,\n",
    "    augment=True\n",
    ")\n",
    "\n",
    "val_generator = DataGenerator(\n",
    "    val_image_ids, \n",
    "    val_pixels,\n",
    "    val_labels, \n",
    "    batch_size=10,\n",
    "    shuffle=False,\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "image_grid = train_generator.image_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLM97xuu_pD6"
   },
   "source": [
    "## Model\n",
    "\n",
    "With the data ready to go I'll define and train the model. As mentioned before this model is heavily inspired by yolo, specifically yolo v3. This is a large and at times complex model. Below is an outline of the model. Basically the model begins with a convolutional layer with 32 filters which doubles in size in the next layer. The filters are then halved in size before doubling every layer up to 128 layers. The filters are then halved again while a larger stride reduces the size of the input image. This pattern of doubling and halving filter sizes continues with a few repeated blocks until we reach a size of 1024. A few resnet skip layers are added in as well to stabilise the large number of layers and reduce the chance of vanishing gradients.\n",
    "\n",
    " An outline of the model taken from the yolov3 [paper](https://pjreddie.com/media/files/papers/YOLOv3.pdf).*\n",
    "\n",
    "Below is my keras implementation of the model. The model is mostly in line with the yolov3 architecture. The main difference is a greater number of max pooling layers and a few layers added on the end to reduce the output shape the desired shape (i.e. a label grid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03sJ5rvF_pD6"
   },
   "outputs": [],
   "source": [
    "x_input = tf.keras.Input(shape=(256,256,3))\n",
    "\n",
    "x = tf.keras.layers.Conv2D(32, (3, 3), strides=(1, 1), padding='same')(x_input)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "x = tf.keras.layers.MaxPooling2D((1, 1), strides=(1, 1))(x)\n",
    "\n",
    "########## block 1 ##########\n",
    "x = tf.keras.layers.Conv2D(64, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "x = tf.keras.layers.MaxPooling2D((1, 1), strides=(1, 1))(x)\n",
    "\n",
    "x_shortcut = x\n",
    "\n",
    "for i in range(2):\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((1, 1), strides=(1, 1))(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((1, 1), strides=(1, 1))(x)\n",
    "\n",
    "    x = tf.keras.layers.Add()([x_shortcut, x])\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    x_shortcut = x\n",
    "\n",
    "\n",
    "########## block 2 ##########\n",
    "x = tf.keras.layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "x = tf.keras.layers.MaxPooling2D((1, 1), strides=(1, 1))(x)\n",
    "\n",
    "x_shortcut = x\n",
    "\n",
    "for i in range(2):\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((1, 1), strides=(1, 1))(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((1, 1), strides=(1, 1))(x)\n",
    "\n",
    "    x = tf.keras.layers.Add()([x_shortcut, x])\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    x_shortcut = x\n",
    "\n",
    "########## block 3 ##########\n",
    "x = tf.keras.layers.Conv2D(256, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "x = tf.keras.layers.MaxPooling2D((1, 1), strides=(1, 1))(x)\n",
    "\n",
    "x_shortcut = x\n",
    "\n",
    "for i in range(8):\n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((1, 1), strides=(1, 1))(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(256, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((1, 1), strides=(1, 1))(x)\n",
    "\n",
    "    x = tf.keras.layers.Add()([x_shortcut, x])\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    x_shortcut = x\n",
    "\n",
    "    \n",
    "########## block 4 ##########\n",
    "x = tf.keras.layers.Conv2D(512, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "x = tf.keras.layers.MaxPooling2D((1, 1), strides=(1, 1))(x)\n",
    "\n",
    "x_shortcut = x\n",
    "\n",
    "for i in range(8):\n",
    "    x = tf.keras.layers.Conv2D(256, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((1, 1), strides=(1, 1))(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(512, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((1, 1), strides=(1, 1))(x)\n",
    "\n",
    "    x = tf.keras.layers.Add()([x_shortcut, x])\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    x_shortcut = x\n",
    "\n",
    "########## block 5 ##########\n",
    "x = tf.keras.layers.Conv2D(1024, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "x = tf.keras.layers.MaxPooling2D((1, 1), strides=(1, 1))(x)\n",
    "\n",
    "x_shortcut = x\n",
    "\n",
    "for i in range(4):\n",
    "    x = tf.keras.layers.Conv2D(512, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((1, 1), strides=(1, 1))(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(1024, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((1, 1), strides=(1, 1))(x)\n",
    "\n",
    "    x = tf.keras.layers.Add()([x_shortcut, x])\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    x_shortcut = x\n",
    "\n",
    "########## output layers ##########\n",
    "x = tf.keras.layers.Conv2D(512, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "x = tf.keras.layers.MaxPooling2D((1, 1), strides=(1, 1))(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(256, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "x = tf.keras.layers.MaxPooling2D((1, 1), strides=(1, 1))(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(128, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "x = tf.keras.layers.MaxPooling2D((1, 1), strides=(1, 1))(x)\n",
    "\n",
    "predictions = tf.keras.layers.Conv2D(10, (1, 1), strides=(1, 1), activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=x_input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZe3Uh0B_pD-"
   },
   "source": [
    "One issue with yolo is that it is likely to contain more cells in its label grid that contain no objects than cells that do contain objects. It is easy then for the model to focus too much on learning to reduce no object cells to zero and not focus enough on getting the bounding boxes to the right shape. To overcome this the yolo [paper](https://pjreddie.com/media/files/papers/YOLOv3.pdf) suggests weighting the cells containing bounding boxes five times higher and the cells with no bounding boxes by half. \n",
    "\n",
    "I have defined a custom loss function to do just this. I have also split the loss function into three parts. The first takes care of the confidence score that is trying to work out if a label grid cell contains a head of wheat or not. Binary cross entropy is used here as that is a binary classification task. The second part looks at the x,y position of the bounding boxes while the third looks at the width,height of the bounding boxes. MSE (mean squared loss) is used for the second and third parts as they are regression tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4zt_tr3_pD_"
   },
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    binary_crossentropy = prob_loss = tf.keras.losses.BinaryCrossentropy(\n",
    "        reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    prob_loss = binary_crossentropy(\n",
    "        tf.concat([y_true[:,:,:,0], y_true[:,:,:,5]], axis=0), \n",
    "        tf.concat([y_pred[:,:,:,0], y_pred[:,:,:,5]], axis=0)\n",
    "    )\n",
    "    \n",
    "    xy_loss = tf.keras.losses.MSE(\n",
    "        tf.concat([y_true[:,:,:,1:3], y_true[:,:,:,6:8]], axis=0), \n",
    "        tf.concat([y_pred[:,:,:,1:3], y_pred[:,:,:,6:8]], axis=0)\n",
    "    )\n",
    "    \n",
    "    wh_loss = tf.keras.losses.MSE(\n",
    "        tf.concat([y_true[:,:,:,3:5], y_true[:,:,:,8:10]], axis=0), \n",
    "        tf.concat([y_pred[:,:,:,3:5], y_pred[:,:,:,8:10]], axis=0)\n",
    "    )\n",
    "    \n",
    "    bboxes_mask = get_mask(y_true)\n",
    "    \n",
    "    xy_loss = xy_loss * bboxes_mask\n",
    "    wh_loss = wh_loss * bboxes_mask\n",
    "    \n",
    "    return prob_loss + xy_loss + wh_loss\n",
    "\n",
    "\n",
    "def get_mask(y_true):\n",
    "    anchor_one_mask = tf.where(\n",
    "        y_true[:,:,:,0] == 0, \n",
    "        0.5, \n",
    "        5.0\n",
    "    )\n",
    "    \n",
    "    anchor_two_mask = tf.where(\n",
    "        y_true[:,:,:,5] == 0, \n",
    "        0.5, \n",
    "        5.0\n",
    "    )\n",
    "    \n",
    "    bboxes_mask = tf.concat(\n",
    "        [anchor_one_mask,anchor_two_mask],\n",
    "        axis=0\n",
    "    )\n",
    "    \n",
    "    return bboxes_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dixcDMXO_pEC"
   },
   "source": [
    "I experimented with a few optimisers including like SGD but in the end Adam was the fastest and most reliable function. I have kept the learning rate reasonably high as it can take a number of steps to get the model moving quickly towards convergence. A higher rate helps to reduce the number of steps needed to get the model going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CybVpoSa_pEC"
   },
   "outputs": [],
   "source": [
    "optimiser = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimiser, \n",
    "    loss=custom_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLLDgZvz_pEF"
   },
   "source": [
    "While a high learning rate at the beginning of a training run is great at the start, it can cause issues as the model approaches convergence when smaller, more careful steps are needed. I considered using learning rate decay to handle this but decided on a callback to reduce the learning rate when it plateaus (or increases) over the space of two epochs. This allows the model to make the most of a higher rate until it that rate is too high at which point the model wuicky reduces it.\n",
    "\n",
    "In addition to this I have added an early stopping callback to stop the model training if is no longer able to reduce the loss. This reduces any waste processing and provides faster feedback if the model just isn't training very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yE0gCHIV_pEG"
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', patience=2, verbose=1),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, verbose=1, restore_best_weights=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzfkQLMo_pEJ"
   },
   "source": [
    "Finally the model is ready to be trained. The data generators are passed into the fit generator method of the model alongside the callbacks and the maximum number of epochs to take. Be warned that with 100 or less images this model can train at an okay speed on CPU. Any more images than that will need the GPU (which could still run for a few hours)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "id": "6odmG_LA_pEJ",
    "outputId": "1100a253-9a82-4376-a6c5-294122c78a60"
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=15,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zmgai3VDE8RI"
   },
   "outputs": [],
   "source": [
    "model.save_weights('wheat_detection_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QL_cPqK_pEM"
   },
   "source": [
    "## Prediction post processing\n",
    "\n",
    "The model outputs the predicted bounding boxes as a label grid. However to visualise the bounding boxes on an image or submit them to the competition the shape for one images bounding boxes need changing from (16,16,10) to (m, 4) where m represents the number of bounding boxes that have a high confidence.\n",
    "\n",
    "This first function transforms the boxes from the yolo format to the coco format. It does this through the following:\n",
    "\n",
    "- return the scale of the boxes from 0-1 to 0-256\n",
    "- change the x,y from the centre of the box to the top left corner\n",
    "- change width and height to x_max, y_max i.e. change to voc shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBKGCdOS_pEN"
   },
   "outputs": [],
   "source": [
    "def prediction_to_bbox(bboxes, image_grid):    \n",
    "    bboxes = bboxes.copy()\n",
    "    \n",
    "    im_width = (image_grid[:,:,2] * 16)\n",
    "    im_height = (image_grid[:,:,3] * 16)\n",
    "    \n",
    "    # descale x,y\n",
    "    bboxes[:,:,1] = (bboxes[:,:,1] * image_grid[:,:,2]) + image_grid[:,:,0]\n",
    "    bboxes[:,:,2] = (bboxes[:,:,2] * image_grid[:,:,3]) + image_grid[:,:,1]\n",
    "    bboxes[:,:,6] = (bboxes[:,:,6] * image_grid[:,:,2]) + image_grid[:,:,0]\n",
    "    bboxes[:,:,7] = (bboxes[:,:,7] * image_grid[:,:,3]) + image_grid[:,:,1]\n",
    "    \n",
    "    # descale width,height\n",
    "    bboxes[:,:,3] = bboxes[:,:,3] * im_width \n",
    "    bboxes[:,:,4] = bboxes[:,:,4] * im_height\n",
    "    bboxes[:,:,8] = bboxes[:,:,8] * im_width \n",
    "    bboxes[:,:,9] = bboxes[:,:,9] * im_height\n",
    "    \n",
    "    # centre x,y to top left x,y\n",
    "    bboxes[:,:,1] = bboxes[:,:,1] - (bboxes[:,:,3] / 2)\n",
    "    bboxes[:,:,2] = bboxes[:,:,2] - (bboxes[:,:,4] / 2)\n",
    "    bboxes[:,:,6] = bboxes[:,:,6] - (bboxes[:,:,8] / 2)\n",
    "    bboxes[:,:,7] = bboxes[:,:,7] - (bboxes[:,:,9] / 2)\n",
    "    \n",
    "    # width,heigth to x_max,y_max\n",
    "    bboxes[:,:,3] = bboxes[:,:,1] + bboxes[:,:,3]\n",
    "    bboxes[:,:,4] = bboxes[:,:,2] + bboxes[:,:,4]\n",
    "    bboxes[:,:,8] = bboxes[:,:,6] + bboxes[:,:,8]\n",
    "    bboxes[:,:,9] = bboxes[:,:,7] + bboxes[:,:,9]\n",
    "    \n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1Jezth5_pEQ"
   },
   "source": [
    "Next the bounding boxes with low confidence need removing. I also need to remove any boxes that overlap another box. Luckily Tensorflow has a non-max suppression function that filters out both low confidence boxes and removes one box if any two overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0-UsZCl_pER"
   },
   "outputs": [],
   "source": [
    "def non_max_suppression(predictions, top_n):\n",
    "    probabilities = np.concatenate((predictions[:,:,0].flatten(), predictions[:,:,5].flatten()), axis=None)\n",
    "    \n",
    "    first_anchors = predictions[:,:,1:5].reshape((16*16, 4))\n",
    "    second_anchors = predictions[:,:,6:10].reshape((16*16, 4))\n",
    "    \n",
    "    bboxes = np.concatenate(\n",
    "        (first_anchors,second_anchors),\n",
    "        axis=0\n",
    "    )\n",
    "    \n",
    "    bboxes, probabilities = select_top(probabilities, bboxes, top_n=top_n)\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "\n",
    "def select_top(probabilities, boxes, top_n=10):\n",
    "    top_indices = tf.image.non_max_suppression(\n",
    "        boxes = boxes, \n",
    "        scores = probabilities, \n",
    "        max_output_size = top_n, \n",
    "        iou_threshold = 0.3,\n",
    "        score_threshold = 0.3\n",
    "    )\n",
    "    \n",
    "    top_indices = top_indices.numpy()\n",
    "    \n",
    "    return boxes[top_indices], probabilities[top_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HB0Sl42U_pET"
   },
   "source": [
    "Wrap these post-processing functions into one and output the predicted bounding boxes as a dictionary where the image id is the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7BmyM5h_pEU"
   },
   "outputs": [],
   "source": [
    "def process_predictions(predictions, image_ids, image_grid):\n",
    "    bboxes = {}\n",
    "    \n",
    "    for i, image_id in enumerate(image_ids):\n",
    "        predictions[i] = prediction_to_bbox(predictions[i], image_grid)\n",
    "        bboxes[image_id] = non_max_suppression(predictions[i], top_n=100)\n",
    "        \n",
    "        # back to coco shape\n",
    "        bboxes[image_id][:,2:4] = bboxes[image_id][:,2:4] - bboxes[image_id][:,0:2]\n",
    "    \n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMkpUKzr_pEW"
   },
   "source": [
    "Let's see how the model did by producing predictions for the training and validation datasets and show these boxes on the first four images from each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4XCLGGNG_pEX"
   },
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(\n",
    "    train_image_ids,\n",
    "    train_pixels,\n",
    "    train_labels, \n",
    "    batch_size=1, \n",
    "    shuffle=False,\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "train_predictions = model.predict(train_generator)\n",
    "train_predictions = process_predictions(train_predictions, train_image_ids, image_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6MzXYwL_pEa"
   },
   "outputs": [],
   "source": [
    "val_predictions = model.predict(val_generator)\n",
    "val_predictions = process_predictions(val_predictions, val_image_ids, image_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPkr17sX_pEd"
   },
   "outputs": [],
   "source": [
    "show_images(train_image_ids[0:4], train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56uEQ2x8F4iA"
   },
   "outputs": [],
   "source": [
    "show_images(val_image_ids[0:4], val_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6PDuAgM_pEh"
   },
   "source": [
    "## Evaluate Model\n",
    "\n",
    "With the model trained it's time to look at the quality of the model. Begin by plotting the loss curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2Mfs8Ih_pEi"
   },
   "outputs": [],
   "source": [
    "print('Epochs: ' + str(len(history.history['loss'])))\n",
    "print('Final training loss: ' + str(history.history['loss'][-1]))\n",
    "print('Final validation loss: ' + str(history.history['val_loss'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LLCqyKaH_pEk"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax[0].set_title('Training Loss')\n",
    "ax[0].plot(history.history['loss'])\n",
    "\n",
    "ax[1].set_title('Validation Loss')\n",
    "ax[1].plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzoMf40oEYWw"
   },
   "source": [
    "Then visualise the first few layers of the model to see how each layer influences the bounding boxes. Start by copying the model and configuring the new one to return each layers output when a prediction is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5q_MlEbDEZJf"
   },
   "outputs": [],
   "source": [
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "evaluation_model = tf.keras.Model(inputs=model.input, outputs=layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ht9ruKt9EivT"
   },
   "source": [
    "Then pick an image and cycle through the layers making a prediction and visualising the features outputted by the layer. The warm colours represent where the features lie in this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIsHtvnbEjt8"
   },
   "outputs": [],
   "source": [
    "image = Image.open(path + train_image_ids[1] + \".jpg\")\n",
    "image = image.resize((256, 256))\n",
    "\n",
    "pixels = np.asarray(image) / 255\n",
    "pixels = np.expand_dims(pixels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghTNGkz1YsUO"
   },
   "outputs": [],
   "source": [
    "num_of_layers = len(layer_outputs)\n",
    "\n",
    "fig, axes = plt.subplots(2, 6, figsize=(20, 10))\n",
    "\n",
    "layer = 0\n",
    "for i in range(0, 2):\n",
    "    for j in range(0, 6):\n",
    "        layer_output = evaluation_model.predict(pixels)[layer]\n",
    "        axes[i, j].imshow(layer_output[0, :, :, 1], cmap='inferno')\n",
    "        \n",
    "        layer = layer + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IChqlHGrYhtp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IEdeo-lfWuwJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23MpbGwyUhFk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Global-Wheat-Detection.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0917a3922b9b40f0929c24c94ff1d18c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0cfab5ab0637424c9ac7f8c0fef5e4cc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b87383d833949f39a7e0baa23092b83": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed3c2c99d4c14a8fb426635085e59dee",
      "max": 3363,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4deacb4d99f84cfd8af95d5a487b7cca",
      "value": 3363
     }
    },
    "46c79133c0f042c1a9d5b811374bc047": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "4deacb4d99f84cfd8af95d5a487b7cca": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "958d613a534645a7ae555407c273691a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_daa1a252e34f4eec9dd733d85bbf6954",
      "placeholder": "​",
      "style": "IPY_MODEL_0917a3922b9b40f0929c24c94ff1d18c",
      "value": " 3363/3363 [31:23&lt;00:00,  1.79it/s]"
     }
    },
    "98764f9cf50545f38d2d601c567fbb72": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9f39bcc48581431aafd02bb8713cbe0a",
       "IPY_MODEL_dfda22bf83524d0ca26f0f3b6d89e051"
      ],
      "layout": "IPY_MODEL_f37cd14598744b1d9c590400ae8054c6"
     }
    },
    "9f0c9acd75714703a2421a8bb03de809": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9f39bcc48581431aafd02bb8713cbe0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed622829fe5944ddb56672caaf4d7c73",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_46c79133c0f042c1a9d5b811374bc047",
      "value": 10
     }
    },
    "daa1a252e34f4eec9dd733d85bbf6954": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dfda22bf83524d0ca26f0f3b6d89e051": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0cfab5ab0637424c9ac7f8c0fef5e4cc",
      "placeholder": "​",
      "style": "IPY_MODEL_9f0c9acd75714703a2421a8bb03de809",
      "value": " 10/10 [00:47&lt;00:00,  4.79s/it]"
     }
    },
    "ed3c2c99d4c14a8fb426635085e59dee": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed622829fe5944ddb56672caaf4d7c73": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f37cd14598744b1d9c590400ae8054c6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f84211a5e13d4c1a8bebc29a6ff76424": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f94a386a48534acdbd2554ae285ca69c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1b87383d833949f39a7e0baa23092b83",
       "IPY_MODEL_958d613a534645a7ae555407c273691a"
      ],
      "layout": "IPY_MODEL_f84211a5e13d4c1a8bebc29a6ff76424"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
